{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理(NLP)實作"
   ]
  },
  {
   "source": [
    "### 自然語言處理主要是指文字(Text)相關的應用\n",
    "\n",
    "- 文字分類(Text Classification)：例如情緒分析(Sentiment Analysis)、主題的分類、垃圾信(Spam)的辨識、...等，乃至於聊天機器人(ChatBot)。\n",
    "- 文字生成(Text Generation)：例如文本摘要(Text Summary)、作詞、作曲、製造假新聞(Fake News)、影像標題(Image captioning)...等。\n",
    "- 翻譯(Text Translation)：多國語言互轉。\n",
    "- 其他：克漏字、錯字更正、命名實體識別（NER）、著作風格的比對，例如紅樓夢最後幾個章節是不是曹雪芹寫的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 程式參考來源：\n",
    "- https://keras.io/api/layers/core_layers/embedding/\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://keras.io/guides/working_with_rnns/\n"
   ]
  },
  {
   "source": [
    "### 簡單 RNN\n",
    "\n",
    "RNN 主要用於時間序列型的資料，如股價、氣候資料，或者上下文相關(Context Sensitive)的資料，例如文章字句有前後關聯，我們需要以較早期發生的資料作為訓練資料，預測當期或未來。\n",
    "\n",
    "因與上下文相關，RNN 的輸入除了特徵(X)外，還會餵入上一筆隱藏層的輸出\n",
    "![](https://ithelp.ithome.com.tw/upload/images/20200925/20001976GzQlGkpwOL.png)\n",
    "\n",
    "當前資料會受到上一筆的影響，上一筆又受到【上上一筆】的影響，類似遞迴的概念，因此，稱為【循環神經網路】(Recurrent Neural Network, RNN)\n",
    "![](https://ithelp.ithome.com.tw/upload/images/20200925/20001976pjlz1ErdbF.png)\n",
    "\n",
    "RNN 基於共享權值(Shared Weights)的假設，遞迴的結果使權值(W)連乘，W>1時，會造成【梯度爆炸】(exploding gradient)，反之，W<1時，則會造成【梯度消失】(vanishing gradient)，故有改良的的演算法如 LSTM(Long Short Term Memory)、GRU(Gated Recurrent Unit)...等，多維護一條【記憶】處理流程。\n",
    "![](https://ithelp.ithome.com.tw/upload/images/20200925/20001976TlanB2yqVi.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "source": [
    "Keras 實作RNN/LSTM/GRU神經層，分別為SimpleRNN/LSTM/GRU，命名空間(Namespace)為 tensorflow.keras.layers，模型結構的第一層必須為嵌入層(Embedding layer)，它將文字轉為緊密的實數空間，使輸入變為向量，才能進行後續的運算。\n",
    "\n",
    "嵌入層(Embedding layer)的重要參數說明如下：\n",
    "\n",
    "- input_dim: int > 0。字彙表大小。\n",
    "- output_dim: int >= 0。詞向量的維度。\n",
    "- input_length: 輸入文字的長度，如果後面接 Flatten 和 Dense 層，則此參數勢必填的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32, 10, 64)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.01953734, -0.0157074 ,  0.00996393, -0.00673544, -0.02587519,\n",
       "         0.03053397, -0.04191018,  0.04229479, -0.01038377,  0.00563044,\n",
       "        -0.00477694,  0.00464076, -0.01616655, -0.04523896, -0.02540561,\n",
       "         0.02133313,  0.00652258,  0.00679905,  0.01225382,  0.0344512 ,\n",
       "        -0.0403166 ,  0.00016652,  0.03814531,  0.02289743,  0.04196259,\n",
       "        -0.00600504,  0.03465677, -0.02778099,  0.03221213, -0.00787107,\n",
       "        -0.04769366,  0.0346876 ,  0.01677204, -0.01855751, -0.00592537,\n",
       "        -0.0323254 , -0.04971549, -0.00043731,  0.00608679, -0.01320542,\n",
       "         0.03071863, -0.01272152, -0.0167508 ,  0.03141818,  0.02908845,\n",
       "         0.01691998, -0.04993988, -0.01873283, -0.02002305,  0.02539049,\n",
       "        -0.01617659, -0.0321295 , -0.02310722, -0.02385777,  0.04154817,\n",
       "        -0.01848627, -0.01114576, -0.0079337 ,  0.0008026 ,  0.021645  ,\n",
       "        -0.0138871 , -0.0208204 , -0.01179491,  0.01101322],\n",
       "       [ 0.03860272, -0.00536673, -0.00034314, -0.01738031, -0.02751308,\n",
       "        -0.01755781, -0.00455065, -0.03396159, -0.03082606,  0.00885979,\n",
       "         0.04824604, -0.02269999, -0.01535629, -0.01140075,  0.02827182,\n",
       "        -0.03355942, -0.04108324,  0.0346159 ,  0.0230307 ,  0.03011003,\n",
       "        -0.02362164, -0.03001687, -0.01716205, -0.00367618,  0.03810057,\n",
       "         0.02316232, -0.01643037, -0.02605412, -0.04352329, -0.02768843,\n",
       "        -0.04046573, -0.03435214, -0.02701786, -0.00937829, -0.01719359,\n",
       "        -0.02724127, -0.00750596, -0.0476433 , -0.03859752,  0.02954019,\n",
       "         0.0420131 , -0.01043917,  0.00289625,  0.02924678,  0.00991429,\n",
       "        -0.02897117,  0.04644588, -0.02619227,  0.00906774, -0.01710214,\n",
       "        -0.04396107, -0.02044681, -0.03891007, -0.01087635, -0.03005244,\n",
       "         0.00687162,  0.01267655, -0.02211848,  0.02390304,  0.03257222,\n",
       "         0.0095517 ,  0.04185628,  0.00909338,  0.00741661],\n",
       "       [ 0.03965594, -0.03084796, -0.01649665,  0.03245098, -0.00536947,\n",
       "         0.00386369,  0.0406199 , -0.03010815, -0.00338111,  0.03543495,\n",
       "        -0.03255757,  0.04239489, -0.02769187, -0.01816312, -0.03998876,\n",
       "         0.0298033 ,  0.04718344, -0.02407429, -0.00654797, -0.04315987,\n",
       "         0.01328498,  0.02642275, -0.02209334,  0.00646309,  0.02487848,\n",
       "        -0.0219908 , -0.04950981,  0.02509833, -0.04687824,  0.04917527,\n",
       "         0.02789317,  0.01941502,  0.01915457,  0.01905173,  0.04893123,\n",
       "         0.03595005, -0.02821933, -0.01841087,  0.03206122, -0.00617137,\n",
       "         0.01279039, -0.0159135 ,  0.01769476,  0.01498507, -0.01588105,\n",
       "         0.03003294, -0.01639774,  0.04259696, -0.02521969,  0.02407398,\n",
       "         0.04255721, -0.03592937,  0.02982405, -0.02777236,  0.0014873 ,\n",
       "         0.02368947,  0.03357241,  0.01500994, -0.02591424,  0.04580962,\n",
       "        -0.01822312,  0.00926638,  0.0136066 , -0.01147419],\n",
       "       [-0.02380061,  0.01538891,  0.0310133 ,  0.00754347,  0.00743908,\n",
       "        -0.02037423,  0.01660294, -0.04829244,  0.04041911,  0.00299012,\n",
       "         0.04200763,  0.02421776,  0.01760808,  0.02589605, -0.00130695,\n",
       "         0.04491131, -0.01245154,  0.00193434,  0.00474969, -0.03848048,\n",
       "         0.02448629,  0.04458723,  0.01420942, -0.03202837,  0.03901762,\n",
       "         0.0240696 ,  0.04546802, -0.03148875, -0.01616429,  0.00567434,\n",
       "        -0.04274442,  0.03796779, -0.04674701,  0.02554737, -0.0377408 ,\n",
       "        -0.04609846,  0.00799258,  0.04222471, -0.04208457,  0.02852619,\n",
       "         0.03962085,  0.02461987,  0.02264848, -0.02607584,  0.03272973,\n",
       "         0.03481958, -0.02152914, -0.01786481,  0.02847855,  0.0231156 ,\n",
       "         0.01699891,  0.04671252, -0.00170271,  0.04478658, -0.04671515,\n",
       "         0.02477118, -0.01150236, -0.00101876, -0.03455748, -0.03024542,\n",
       "         0.03007421,  0.04478158,  0.00671996,  0.0065439 ],\n",
       "       [-0.01803197, -0.01813557, -0.02197447,  0.04757375,  0.030317  ,\n",
       "         0.04794372,  0.03555829,  0.01761082,  0.02616758,  0.0125511 ,\n",
       "        -0.01041361, -0.00038682,  0.01215506,  0.02689213, -0.03486334,\n",
       "         0.00722218, -0.0312094 ,  0.03431152,  0.03381412,  0.04770411,\n",
       "         0.04648418,  0.02170802, -0.00316032,  0.0261403 , -0.01490836,\n",
       "         0.04115487, -0.01524119,  0.01183166, -0.042372  , -0.04272419,\n",
       "        -0.03510552,  0.04883534,  0.01187204, -0.0198779 , -0.00840336,\n",
       "        -0.04844568,  0.04235622, -0.01524252,  0.00766136,  0.00233426,\n",
       "        -0.00028897,  0.040358  ,  0.03630065, -0.01244508, -0.00342876,\n",
       "        -0.03129907,  0.041971  , -0.01365057, -0.00458499, -0.03773701,\n",
       "         0.04399521, -0.01900516, -0.0255888 , -0.03003812,  0.02019087,\n",
       "        -0.04426486,  0.03150262, -0.00072034, -0.01448697,  0.03342134,\n",
       "         0.04529153,  0.02567765,  0.03230837,  0.02723931],\n",
       "       [ 0.04483153, -0.03340818,  0.03098811, -0.03371134, -0.02946171,\n",
       "         0.00949862, -0.0334512 , -0.04903556,  0.03719827, -0.04206418,\n",
       "        -0.02932446,  0.04271678, -0.01980114,  0.00122916, -0.03865107,\n",
       "         0.01487695, -0.01867257, -0.03904139,  0.04173608, -0.01012744,\n",
       "        -0.02994568, -0.00752643, -0.02044828, -0.01588343, -0.0462874 ,\n",
       "         0.03266349,  0.01199002, -0.0109673 , -0.04467769,  0.04668062,\n",
       "        -0.03987791, -0.02687632, -0.02301397, -0.02316419,  0.03923703,\n",
       "         0.01299727,  0.0153234 , -0.04892061,  0.04972067, -0.03263171,\n",
       "        -0.0029096 , -0.0062701 ,  0.03334154, -0.01626283,  0.01478278,\n",
       "         0.01061348,  0.02792211, -0.03913348, -0.02406398,  0.00459655,\n",
       "         0.01277341,  0.01646939, -0.02708435, -0.02823509, -0.03922744,\n",
       "        -0.00637542, -0.02743138,  0.02464697, -0.028113  ,  0.03654493,\n",
       "        -0.02665112,  0.02334881, -0.04465476,  0.04164742],\n",
       "       [ 0.01933682,  0.01045232,  0.00895434, -0.04897994, -0.00886229,\n",
       "        -0.02764578, -0.04212658,  0.02341082,  0.0061653 ,  0.03332346,\n",
       "        -0.04728324, -0.00831531, -0.01528784,  0.02137924,  0.00829886,\n",
       "        -0.0205867 , -0.03312397,  0.02780053, -0.02194016,  0.04125923,\n",
       "         0.0201954 ,  0.02899975,  0.0102298 , -0.01465024, -0.02630928,\n",
       "         0.04342545, -0.01005846, -0.03080354, -0.03034508,  0.03499976,\n",
       "        -0.04146799,  0.03346021,  0.03217963,  0.00364166,  0.00331367,\n",
       "         0.00973662, -0.00876409, -0.0234247 ,  0.00049838, -0.01740066,\n",
       "         0.03247991, -0.01807557, -0.02669755,  0.00546652,  0.04937987,\n",
       "         0.02250934,  0.02170969, -0.03440964, -0.01467923,  0.01923296,\n",
       "         0.01124469,  0.01780378,  0.01270732,  0.0375115 ,  0.01811234,\n",
       "        -0.01597574, -0.04619491,  0.03526232, -0.01885612, -0.00900283,\n",
       "        -0.00791126, -0.03641151,  0.02970116, -0.0371429 ],\n",
       "       [-0.00164795,  0.04248444,  0.04780943, -0.04235297, -0.02134236,\n",
       "         0.01366241,  0.03283683,  0.04210845,  0.04099253, -0.01961809,\n",
       "         0.04132823,  0.00948811,  0.00099344,  0.0294737 ,  0.01359198,\n",
       "        -0.01581166,  0.04752601,  0.0196886 , -0.00384549,  0.03500246,\n",
       "         0.00205004, -0.04394674, -0.00780505,  0.02094446, -0.02057168,\n",
       "        -0.00597922, -0.03008587, -0.0356065 , -0.04657084, -0.00782035,\n",
       "         0.03793031,  0.03143866, -0.02569417, -0.04586339,  0.03482476,\n",
       "         0.04627847,  0.0025797 , -0.03541968,  0.00252189,  0.04086905,\n",
       "        -0.01631347, -0.04936351,  0.03771658,  0.00591069, -0.01713794,\n",
       "        -0.03175   , -0.03250705,  0.00973643,  0.0208887 ,  0.03447502,\n",
       "        -0.00184606, -0.02027568,  0.04227247,  0.04285877,  0.03987778,\n",
       "         0.01897415, -0.01870923, -0.0169868 ,  0.04652416,  0.02010876,\n",
       "        -0.02981759, -0.02157409, -0.04399332, -0.03474343],\n",
       "       [ 0.01953734, -0.0157074 ,  0.00996393, -0.00673544, -0.02587519,\n",
       "         0.03053397, -0.04191018,  0.04229479, -0.01038377,  0.00563044,\n",
       "        -0.00477694,  0.00464076, -0.01616655, -0.04523896, -0.02540561,\n",
       "         0.02133313,  0.00652258,  0.00679905,  0.01225382,  0.0344512 ,\n",
       "        -0.0403166 ,  0.00016652,  0.03814531,  0.02289743,  0.04196259,\n",
       "        -0.00600504,  0.03465677, -0.02778099,  0.03221213, -0.00787107,\n",
       "        -0.04769366,  0.0346876 ,  0.01677204, -0.01855751, -0.00592537,\n",
       "        -0.0323254 , -0.04971549, -0.00043731,  0.00608679, -0.01320542,\n",
       "         0.03071863, -0.01272152, -0.0167508 ,  0.03141818,  0.02908845,\n",
       "         0.01691998, -0.04993988, -0.01873283, -0.02002305,  0.02539049,\n",
       "        -0.01617659, -0.0321295 , -0.02310722, -0.02385777,  0.04154817,\n",
       "        -0.01848627, -0.01114576, -0.0079337 ,  0.0008026 ,  0.021645  ,\n",
       "        -0.0138871 , -0.0208204 , -0.01179491,  0.01101322],\n",
       "       [-0.02462723, -0.01121212, -0.0237769 ,  0.01571739,  0.04176654,\n",
       "        -0.01857372, -0.00141487, -0.02639989,  0.0437325 , -0.02499118,\n",
       "        -0.03502844,  0.0440847 ,  0.02387917,  0.0039168 , -0.00947361,\n",
       "         0.02993795,  0.03099826, -0.02587333, -0.02205601, -0.02787867,\n",
       "         0.03685763, -0.01436716, -0.04505787,  0.01890026,  0.01069188,\n",
       "        -0.0251238 , -0.00956897, -0.01950482, -0.00722382, -0.01587985,\n",
       "         0.00666589, -0.0073369 , -0.00675081, -0.00371303,  0.04068594,\n",
       "         0.02985828, -0.0075051 , -0.03926507,  0.01907576,  0.03578389,\n",
       "        -0.01338113,  0.01430957, -0.04571489, -0.04433345,  0.04712835,\n",
       "        -0.03091985, -0.00844755, -0.03277943, -0.00260588,  0.02167634,\n",
       "         0.00060435, -0.01005709,  0.00197799, -0.0472608 , -0.03850852,\n",
       "         0.00435536,  0.00863545, -0.03500068, -0.03379698, -0.02902678,\n",
       "        -0.01475571, -0.0051466 , -0.02982444,  0.04873002]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# 字彙表最大為1000，輸出維度為 64，輸入的字數為 10\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# 產生亂數資料，32筆資料，每筆 10 個數字\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "# 指定損失函數\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "# 預測\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "output_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用真實的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 4, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 測試資料\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "\n",
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "\n",
    "# 先轉成 one-hot encoding\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "\n",
    "# 轉成固定長度，長度不足則後面補空白\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "\n",
    "# 模型只有 Embedding\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 64, input_length=maxlen))\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "# 預測\n",
    "output_array = model.predict(padded_docs)\n",
    "output_array.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加上Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels(1：正面、0：負面)\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 8, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "# 加上一般的完全連接層(Dense)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加上 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               17536     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 18,065\n",
      "Trainable params: 18,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels(1：正面、0：負面)\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 8, input_length=maxlen))\n",
    "# Add a RNN layer with 128 internal units.\n",
    "model.add(layers.SimpleRNN(128))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用詞向量(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取 GloVe 100維的詞向量，產生字典資料型的變數，方便搜尋\n",
    "\n",
    "[檔案下載位置](http://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./glove/glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.array(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "maxlen = 4\n",
    "\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels(1：正面、0：負面)\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 轉換為GloVe 100維的詞向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 設為不需訓練，直接輸入轉換後的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 4, 100)            5000      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 122,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 5,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# trainable=False\n",
    "model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5e61f42cb8918a69a5fd4699f126ba1b36015307b65c3d7b25dba18894b1b545"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}